{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Web7.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julichitai/Ant-Colony-Optimization/blob/master/neural_nets/web7/Web7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtrpTCq95eOC"
      },
      "source": [
        "# Введение в искусственные нейронные сети\n",
        "# Урок 7. Дектирование объектов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2dU0-265eOE"
      },
      "source": [
        "## Содержание методического пособия:\n",
        "\n",
        "\n",
        "<ol>\n",
        "<li>Что такое дектирование объектов на изображении</li>\n",
        "<li>Виды архитектур для дектирования объектов на изображении</li>\n",
        "<li>Практический пример сегментации</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd2MRCG85eOF"
      },
      "source": [
        "## Что такое детектирование объектов на изображении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d_wRv5M5eOG"
      },
      "source": [
        "Дектирование объектов на изображениях, пожалуй одна из самых сложных и самых полезных задач, которую может решать компьютерное зрение. \n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=14lTv3rt5CCdTamrpgEau21HFdztKvEaR'>\n",
        "\n",
        "Сложность проистекает из того факта, что нам нужно не только опредилить какому классу пренадлежат объекты на изображении, отделить одни экзепляры классов от других, но и найти их месторасположение на изображении.\n",
        "\n",
        "Полезность решения данного задачи связана с тем фактом, что она приближена к возможностям человеческого зрения и восприятия. Мы смотрим на окружающий мир, видем те или иные объекты и понимаем, где они находятся. А понимая, где они находятся мы можем понимать, как они движутся. Нейронные сети решающие подобные задачи могут применяться в самом широком спектре областей - начиная от систем видеонаблюдения до полностью автоматизированных магазинов наподобие Amazon Go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCKq4iSy5eOI"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1PqO07j10VvJzg2TXpELU56KI1Ygp_bTm'>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1MifpSYaLffYwVFwpo_2NdW-VQnRmRLfD'>\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAUdlWEK5eOJ"
      },
      "source": [
        "В силу того, что решение данной задачи существенно отличается от задачи классификации изображений, существенно отличаются и архитектуры нейронных сетей для этой задачи, которые мы разберем на данном уроке. В первую очередь мы укажем на отличия в датасетах и метриках. \n",
        "\n",
        "Для обучения подобных нейронных сетей нам нужно не только понимать какие классы присутсвуют на изображении, но и понимать где они находятся, поэтому в датасетах для object detection, выделяются также bounding box'ы для того, чтобы нейросеть могла корректировать свои предсказания месторасположения объектов по ходу обучения.\n",
        "\n",
        "Известными датасетами для object detection являются следующие:\n",
        "1. [The PASCAL VOC'12](http://host.robots.ox.ac.uk/pascal/VOC/)\n",
        "    - 20 классов\n",
        "    - Train + Val содержат вместе 11,530 картинок с 27,450 ROI-боксов и 6,929 сегментаøий\n",
        "    - Картинки бывают разных размеров и качества\n",
        "    - Статья: [http://host.robots.ox.ac.uk/pascal/VOC/pubs/ever](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf)\n",
        "    - Картинки изначально были взяты с сайта [https://www.flickr.com/](https://www.flickr.com/)\n",
        "2. [MS COCO dataset'17](https://cocodataset.org/)\n",
        "    - MicroSoft Common Objects in COntext\n",
        "    - 80 классов\n",
        "    - Train: 118K (18GB) картинок\n",
        "    - Val: 5K (1GB) картинок\n",
        "    - Test: 41K (6GB) картинок\n",
        "    - Картинки бывают разных размеров и качества\n",
        "    - Статья: [https://arxiv.org/pdf/1405.0312.pdf](https://arxiv.org/pdf/1405.0312.pdf)\n",
        "3. [ImageNet](http://image-net.org/download-bboxes)\n",
        "    - 200 классов\n",
        "    - В Train + Val вместе 400,000 картинок, взятых из ImageNet, 350,000 bounding box’ов размечено\n",
        "    - Картинки бывают разных размеров и качества\n",
        "    - Статьи: [http://image-net.org/about-publication](http://image-net.org/about-publication)\n",
        "    - Загрузить object detection-часть датасета: [http://image-net.org/download-bboxes](http://image-net.org/download-bboxes)\n",
        "    - Редко используют в сравнении в статьях\n",
        "4. [Google Open Images v4'18](https://storage.googleapis.com/openimages/web/index.html)\n",
        "    - 600 классов\n",
        "    - Train: 1743K картинок\n",
        "    - Val: 41K картинок\n",
        "    - Test: 125K картинок\n",
        "    - Подробное описание датасета: [https://storage.googleapis.com/openimages/web/factsfigures.html](https://storage.googleapis.com/openimages/web/factsfigures.html)\n",
        "    - Kaggle-соревнование: [https://www.kaggle.com/c/google-ai-open-images-object-detection-track](https://www.kaggle.com/c/google-ai-open-images-object-detection-track)\n",
        "5. [Epic Kitchens](https://epic-kitchens.github.io/2018)\n",
        "    - 300 классов\n",
        "    - 32 кухни - 4 города\n",
        "    - GoPro на голове\n",
        "    - 55 часов Full HD видео в 60fps\n",
        "    - 11.5M кадров\n",
        "    - 39,594 action сегментов\n",
        "    - 454,158 object bounding box’ов\n",
        "    - Описание датасета: [https://epic-kitchens.github.io/2018](https://epic-kitchens.github.io/2018)\n",
        "    - Статья: [https://arxiv.org/pdf/1804.02748.pdf](https://arxiv.org/pdf/1804.02748.pdf)\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1qFR_GFizsQEFyaxa90CfxioC2BT_FBSH' width=300>\n",
        "\n",
        "Разметка может быть в разных форматах:\n",
        "\n",
        "- pascal_voc: min/max [x_min, y_min, x_max, y_max]\n",
        "- coco: width/height [x_min, y_min, width, height]\n",
        "\n",
        "Также стоит отметить, что если в случае с задачей классификации, нам было возможно оценить работу нейронной сети, просто определив верный или неверный класс был предсказан, то в случае с object detection нам нужно определить помимо классов, насколько точно определено его местоположение на изображении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sKmio7ZFgpZ"
      },
      "source": [
        "## Метрики для задач детекции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7QKuD6ZU1mG"
      },
      "source": [
        "### **Intersection over Union (IoU)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4psTuLXFtJU"
      },
      "source": [
        "\n",
        "IoU - это мера величины перекрытия между двумя прямоугольниками). Он вычисляет размер перекрытия между двумя объектами, деленный на общую площадь двух объединенных объектов.\n",
        "\n",
        "Его можно представить следующим образом:\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1X_rEwPnYi2p-RYYS8yjwV96vRxlZ6Kwi'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh0RJql0HfC6"
      },
      "source": [
        "def calculate_iou(gt, pr) -> float:\n",
        "    \"\"\" pascal VOC format [xmin, ymin, xmax, ymax] \"\"\"\n",
        "\n",
        "    # Calculate overlap area\n",
        "    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n",
        "    \n",
        "    if dx < 0:\n",
        "        return 0.0\n",
        "    \n",
        "    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n",
        "\n",
        "    if dy < 0:\n",
        "        return 0.0\n",
        "\n",
        "    overlap_area = dx * dy\n",
        "\n",
        "    # Calculate union area\n",
        "    union_area = (\n",
        "            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n",
        "            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n",
        "            overlap_area\n",
        "    )\n",
        "\n",
        "    return overlap_area / union_area"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBtL-vdsHu65"
      },
      "source": [
        "from scipy import misc\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR6t0DH_H4Hn"
      },
      "source": [
        "img = misc.face()\n",
        "\n",
        "# pascal voc format [xmin, ymin, xmax, ymax]\n",
        "gt = [400, 50, 900, 500]\n",
        "\n",
        "start_point, end_point = (gt[0], gt[1]), (gt[2], gt[3])\n",
        "thickness = 5\n",
        "  \n",
        "image = cv2.rectangle(img, start_point, end_point,\n",
        "                      color=(255, 0, 0), thickness=thickness)\n",
        "  \n",
        "\n",
        "# prediction\n",
        "pred = [450, 100, 950, 550]\n",
        "\n",
        "start_point, end_point = (pred[0], pred[1]), (pred[2], pred[3])  \n",
        "\n",
        "image_pred = cv2.rectangle(image, start_point, end_point,\n",
        "                           color=(0, 0, 255), thickness=thickness)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(image_pred)\n",
        "\n",
        "print(f'IoU {calculate_iou(gt, pred)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nixs3V2eKcxr"
      },
      "source": [
        "img = misc.face()\n",
        "\n",
        "# pascal voc format [xmin, ymin, xmax, ymax]\n",
        "gt = [400, 50, 900, 500]\n",
        "\n",
        "start_point, end_point = (gt[0], gt[1]), (gt[2], gt[3])\n",
        "thickness = 5\n",
        "  \n",
        "image = cv2.rectangle(img, start_point, end_point,\n",
        "                      color=(255, 0, 0), thickness=thickness)\n",
        "  \n",
        "\n",
        "# prediction\n",
        "pred = [600, 100, 1100, 600]\n",
        "\n",
        "start_point, end_point = (pred[0], pred[1]), (pred[2], pred[3])  \n",
        "\n",
        "image_pred = cv2.rectangle(image, start_point, end_point,\n",
        "                           color=(0, 0, 255), thickness=thickness)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(image_pred)\n",
        "\n",
        "print(f'IoU {calculate_iou(gt, pred)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUyFSpqtLsXl"
      },
      "source": [
        "img = misc.face()\n",
        "\n",
        "# pascal voc format [xmin, ymin, xmax, ymax]\n",
        "gt = [400, 50, 900, 500]\n",
        "\n",
        "start_point, end_point = (gt[0], gt[1]), (gt[2], gt[3])\n",
        "thickness = 5\n",
        "  \n",
        "image = cv2.rectangle(img, start_point, end_point,\n",
        "                      color=(255, 0, 0), thickness=thickness)\n",
        "  \n",
        "\n",
        "# prediction\n",
        "pred = [10, 10, 1000, 750]\n",
        "\n",
        "start_point, end_point = (pred[0], pred[1]), (pred[2], pred[3])  \n",
        "\n",
        "image_pred = cv2.rectangle(image, start_point, end_point,\n",
        "                           color=(0, 0, 255), thickness=thickness)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(image_pred)\n",
        "\n",
        "print(f'IoU {calculate_iou(gt, pred)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW_-1SB1U5ED"
      },
      "source": [
        "### **mAP (mean Average Precision)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQZgKoPk6DfW"
      },
      "source": [
        "Более подробно можете почитать [здесь](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173).\n",
        "\n",
        "**mean Average Precision (mAP)** - среднее значение максимумов precision'ов при различных значениях recall'ов.\n",
        "\n",
        "При каждом пороговом значении t вычисляется значение точности, основанное на количестве истинных положительных результатов (TP), ложных отрицательных результатов (FN) и ложных положительных результатов (FP), полученных в результате сравнения предсказанного объекта со всеми основными объектами истинности:\n",
        "\n",
        "\n",
        "**True Positive** подсчитывается, когда предсказанный bounding box совпадает с истинным bounding box с IoU выше трешхолда.\n",
        "\n",
        "**False Positive** указывает на то, что предсказанный bounding box не имел связанного с ним объекта ground truth.\n",
        "\n",
        "**False Negative** указывает на то, что объект ground truth не имеет связанного предсказанного bounding box.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1rMpJT4tlWCzLpT2B_xf5a1HCqsYHFx5d'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wnSozq6Tj6o"
      },
      "source": [
        "$$Precision = \\frac{TP}{TP + FP}$$\n",
        "$$Recall = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "Средняя точность одного изображения вычисляется как среднее значение приведенных выше значений точности при каждом пороге IoU:\n",
        "\n",
        "$$ AP = \\frac{1}{11} (AP_r(0)+AP_r(0.1)+...+AP_r(1)) = \\frac{1}{11}(5 * 1 + 4 * 0.57 + 2 * 0.5)$$\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1_3_gvhGivTP-V1o8-WGcS7JRgQDt5W23'>\n",
        "\n",
        "mAP - среднее AP по всем классам.\n",
        "\n",
        "\n",
        "Здесь представлены работа двух теоретических детектора на двух картинках и их mAP будут одинковыми, хотя очевидно, что второй детектор выдает ненужные bounding box'ы для каждого объекта, что показатель не самой лучшей детекции.\n",
        " \n",
        "<img src='https://drive.google.com/uc?export=view&id=1MIIVJN1Tov-itBIPB-mLkdZrlBZ7BHhK'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqs1JLvx5eOL"
      },
      "source": [
        "# Виды архитектур для детектирования объектов на изображении.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7WsQEsl5eOM"
      },
      "source": [
        "[Репозиторий со списком детекторов](https://github.com/hoya012/deep_learning_object_detection).\n",
        "\n",
        "<img src='https://drive.google.com/uc?exportr=view&id=1S_vzpElPE0zzu7Q3VFiDlxyWcMeB8Vk-'>\n",
        "\n",
        "Начиная с 2012 г., когда широкие круги специалистов в области компьютерного зрения стали использовать нейронные сети, для тех задач, которые прежде решали классические алгоритмы комп зрения, появилось и продолжает появляться множество архитектур предназначенных для object detection. Мы осветим несколько архитектур, которые стали важными вехами с одной стороны, а с другой стороны их модификации являются на данный момент лучшими способами для решения задачи object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrSDvAKS5eOO"
      },
      "source": [
        "## R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcVVnBz15eOP"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1xLEUoJNXrAbBIQOp0rbmPuchXkRXLj8N'>\n",
        "\n",
        "R-CNN - это одна из первых архитектур для решения задачи object detection на основе сверточных нейронных сетей.\n",
        "Данная архитектура состоит из трех частей:\n",
        "\n",
        "1. Классический алгоритм компьютерного зрения, который находит области изображения, на которых потенциально могут содержаться объекты\n",
        "\n",
        "2. Сверточная нейронная сеть, которая запускается по-отдельности в каждом найденном регионе и выдает набор фичей.\n",
        "\n",
        "3. SVM алгоритм, который обучается на этих фичах определять те или иные классы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyehM5nIfob4"
      },
      "source": [
        "## Fast R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAtlXx075eOQ"
      },
      "source": [
        "R-CNN работала медленно. Это происходило из-за того, что приходилось многократно запускать сверточную часть на множестве областей изображения. На одно изображение уходили десятки секунд. Также данная архитектура не могла обучаться целиком, а обучалась по-отдельности. Кроме того обучалась данная нейросеть тоже медленно. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-IS6-8_5eOa"
      },
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1BVfWnSccmcrX6g6HjOA5gWATXDZTcASs'>\n",
        "\n",
        "Данная архитектура появилась в 2015 г. и была призвана решить упомянутые выше проблемы R-CNN. Fast R-CNN состоял уже из следующих компонентов:\n",
        "\n",
        "1. Сверточная нейронная сеть которая запускается один раз на всем изображении.\n",
        "\n",
        "2. ROI - компонент, который позволяет искать области, где могут находиться объекты не на первоначальных пикслелях изображения, а на карте признаков, которую выдает сверточная часть данной архитектуры.\n",
        "\n",
        "3. Полносвязный слой, который делает непосредственно предсказание.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfs0Ib_o5eOe"
      },
      "source": [
        "Данная архитектура ускорила и обучение и скорость работы изначальной архитектуры R-CNN существенно. Теперь одно изображение обрабатывалось нейронной сетью за 2 секунды. Однако это по-прежнему было далеко от real-time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DoRog05giD0"
      },
      "source": [
        "## Faster R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1G6E_z85eOh"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1F4SgQxscVL3S8Vva_R8VEh2cTfqCQlv-'>\n",
        "\n",
        "Данная архитектура появилась в 2016 году и является как одной из самых точных на сегодняшний день, так и относительно быстрой, время обработки ею изображения занимает меньше секунды.\n",
        "\n",
        "Усовершенствование произошло из-за переработки генерации регионов, потому что этот процесс всё еще занимал приличное время, он был основан не на нейросетевых подходах, а вот в Faster R-CNN данный этап перешел на нейронную сеть, которая обучается находить правильные регионы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enic02Ss5eOi"
      },
      "source": [
        "Она состоит из следующих компонентов:\n",
        "1. Сверточная нейронная сеть\n",
        "2. Нейронная сеть, корректирующая работу ROI, выбирающего регионы в карте признаков, получающейся в ходе работы сверточной части.\n",
        "3. Полносвязные слои осуществляющие предсказание."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjm4NnX5eOn"
      },
      "source": [
        "Данная архитектура позволяла окончательно позволили обучать нейроннуют сеть для object detection end-to-end, т.е. нейронная сеть училась и определять классы объектов и корректировать предсказания местоположения объектов -(bounding box'ы)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFQrI8Nz5eOo"
      },
      "source": [
        "## YOLO (You Only Look Once)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGZ6cIVu5eOq"
      },
      "source": [
        "[Подробный разбор сети](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/).\n",
        "\n",
        "Несмотря на то, что Faster R-CNN работала относительно быстро и с высокой точностью, она все таки была тяжеловестна и недостаточно быстра для многих задач. Все вышеприведенные архитектуры относиться к двухстадийным архитектурам, т.е. мы отдельно находим объекты и отдельно их классифицируем.\n",
        "\n",
        "Архитектура YOLO, появившаяся в 2016 г. является первой популярной одностадийной архитектурой. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkTLNyS45eOu"
      },
      "source": [
        "Данная нейронная сеть разбивает все изображение на фиксированное количество квадратов. Затем за один проход она пытается предсказать в разных комбинациях этих квадратов те или иные классы. Таким образом данная нейронная сеть несколько теряет в точности, но существенно приобретает в скорости работы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMCgNQX15eOv"
      },
      "source": [
        "# SSD (Single Shot MultiBox Detector) #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSb2p5VA5eOy"
      },
      "source": [
        "Данная архитектура появилась в 2016 г. и различные ее модификации являются одними из самыми применяемых на практике.\n",
        "\n",
        "Она также как и YOLO является одностадийным дектором, также как и YOLO пытается на лету определить boundig box'ы и классы, но считывание ее результатов происходит на разных масштабах в конце нейронной сети, подобному так как это происходит в архитектуре FPN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAoLxI1Z5eOz"
      },
      "source": [
        "![SSD.png](attachment:SSD.png)\n",
        "Источник изображения: https://arxiv.org/pdf/1512.02325.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufo-00oW5eO2"
      },
      "source": [
        "### Mask R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mgtGF1C5eO4"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=v&id=17KahvDOKZasZrU8G8u5bTzScX5NZwWmw'>\n",
        "\n",
        "Mask R-CNN позволяет решить задачу instance segmentation, которую мы рассматривали на предыдущем уроке. Данная нейросеть сначала посредством Fast R-CNN находит нужны объекты на изображении, а затем посредством сегментации накладывает на них маску."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZRIbJdH5eO5"
      },
      "source": [
        "![Mask_R-CNN.png](attachment:Mask_R-CNN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY6RAy-H5eO7"
      },
      "source": [
        "Источник изображения: https://miro.medium.com/max/1908/1*ui1roGvi_F77TY07PdaI8w.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZCM65CBt1CJ"
      },
      "source": [
        "В целом, архитектуру для решения задачи object detection нужно подбирать исходя из нужного вам сочетания точности распознования и скорости работы нейронной сети. \n",
        "\n",
        "Данная диаграмма показывая различные архитектуры с различными вариантами сверточной части для них, дает общее представление в этом вопросе - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hlrZAw65eO9"
      },
      "source": [
        "![compare_table.png](attachment:compare_table.png)\n",
        "Источник изображения: https://cdn-images-1.medium.com/fit/t/3750/2265/1*xgBs8CZdf1AvaFz92ERB6A.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eGMFag-5eO_"
      },
      "source": [
        "## Практический пример "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1pCjWWR5ePB"
      },
      "source": [
        "Обучение нейронных сетей для object detection требует работы над большими датасетами и громоздкой архитектуры. Мы приведем пример того, как может быть определена нейросеть SSD на tensorflow 1.15. Полный код для обучения данной нейронной сети можно найти в данном репозитории - https://github.com/sergeyveneckiy/ssd-tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxeknW6OJMgW"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD60EbcAQqov"
      },
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# Author: Lukasz Janyst <lukasz@jany.st>\n",
        "# Date:   27.08.2017\n",
        "#-------------------------------------------------------------------------------\n",
        "# This file is part of SSD-TensorFlow.\n",
        "#\n",
        "# SSD-TensorFlow is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# SSD-TensorFlow is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with SSD-Tensorflow.  If not, see <http://www.gnu.org/licenses/>.\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from tqdm import tqdm\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "class DLProgress(tqdm):\n",
        "    last_block = 0\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def conv_map(x, size, shape, stride, name, padding='SAME'):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable(\"filter\",\n",
        "                            shape=[shape, shape, x.get_shape()[3], size],\n",
        "                            initializer=tf.contrib.layers.xavier_initializer())\n",
        "        b = tf.Variable(tf.zeros(size), name='biases')\n",
        "        x = tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding=padding)\n",
        "        x = tf.nn.bias_add(x, b)\n",
        "        x = tf.nn.relu(x)\n",
        "        l2 = tf.nn.l2_loss(w)\n",
        "    return x, l2\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def classifier(x, size, mapsize, name):\n",
        "    with tf.variable_scope(name):\n",
        "        w = tf.get_variable(\"filter\",\n",
        "                            shape=[3, 3, x.get_shape()[3], size],\n",
        "                            initializer=tf.contrib.layers.xavier_initializer())\n",
        "        b = tf.Variable(tf.zeros(size), name='biases')\n",
        "        x = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        x = tf.nn.bias_add(x, b)\n",
        "        x = tf.reshape(x, [-1, mapsize.w*mapsize.h, size])\n",
        "        l2 = tf.nn.l2_loss(w)\n",
        "    return x, l2\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def smooth_l1_loss(x):\n",
        "    square_loss   = 0.5*x**2\n",
        "    absolute_loss = tf.abs(x)\n",
        "    return tf.where(tf.less(absolute_loss, 1.), square_loss, absolute_loss-0.5)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def array2tensor(x, name):\n",
        "    init = tf.constant_initializer(value=x, dtype=tf.float32)\n",
        "    tensor = tf.get_variable(name=name, initializer=init, shape=x.shape)\n",
        "    return tensor\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "def l2_normalization(x, initial_scale, channels, name):\n",
        "    with tf.variable_scope(name):\n",
        "        scale = array2tensor(initial_scale*np.ones(channels), 'scale')\n",
        "        x = scale*tf.nn.l2_normalize(x, axis=-1)\n",
        "    return x\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "class SSDVGG:\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __init__(self, session, preset):\n",
        "        self.preset = preset\n",
        "        self.session = session\n",
        "        self.__built = False\n",
        "        self.__build_names()\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_from_vgg(self, vgg_dir, num_classes, a_trous=True,\n",
        "                       progress_hook='tqdm'):\n",
        "        \"\"\"\n",
        "        Build the model for training based on a pre-define vgg16 model.\n",
        "        :param vgg_dir:       directory where the vgg model should be stored\n",
        "        :param num_classes:   number of classes\n",
        "        :param progress_hook: a hook to show download progress of vgg16;\n",
        "                              the value may be a callable for urlretrieve\n",
        "                              or string \"tqdm\"\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes+1\n",
        "        self.num_vars = num_classes+5\n",
        "        self.l2_loss = 0\n",
        "        self.__download_vgg(vgg_dir, progress_hook)\n",
        "        self.__load_vgg(vgg_dir)\n",
        "        if a_trous: self.__build_vgg_mods_a_trous()\n",
        "        else: self.__build_vgg_mods()\n",
        "        self.__build_ssd_layers()\n",
        "        self.__build_norms()\n",
        "        self.__select_feature_maps()\n",
        "        self.__build_classifiers()\n",
        "        self.__built = True\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_from_metagraph(self, metagraph_file, checkpoint_file):\n",
        "        \"\"\"\n",
        "        Build the model for inference from a metagraph shapshot and weights\n",
        "        checkpoint.\n",
        "        \"\"\"\n",
        "        sess = self.session\n",
        "        saver = tf.train.import_meta_graph(metagraph_file)\n",
        "        saver.restore(sess, checkpoint_file)\n",
        "        self.image_input = sess.graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob   = sess.graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.result      = sess.graph.get_tensor_by_name('result/result:0')\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_optimizer_from_metagraph(self):\n",
        "        \"\"\"\n",
        "        Get the optimizer and the loss from metagraph\n",
        "        \"\"\"\n",
        "        sess = self.session\n",
        "        self.loss = sess.graph.get_tensor_by_name('total_loss/loss:0')\n",
        "        self.localization_loss = sess.graph.get_tensor_by_name('localization_loss/localization_loss:0')\n",
        "        self.confidence_loss = sess.graph.get_tensor_by_name('confidence_loss/confidence_loss:0')\n",
        "        self.l2_loss = sess.graph.get_tensor_by_name('total_loss/l2_loss:0')\n",
        "        self.optimizer = sess.graph.get_operation_by_name('optimizer/optimizer')\n",
        "        self.labels = sess.graph.get_tensor_by_name('labels:0')\n",
        "\n",
        "        self.losses = {\n",
        "            'total': self.loss,\n",
        "            'localization': self.localization_loss,\n",
        "            'confidence': self.confidence_loss,\n",
        "            'l2': self.l2_loss\n",
        "        }\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __download_vgg(self, vgg_dir, progress_hook):\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Check if the model needs to be downloaded\n",
        "        #-----------------------------------------------------------------------\n",
        "        vgg_archive = 'vgg.zip'\n",
        "        vgg_files   = [\n",
        "            vgg_dir + '/variables/variables.data-00000-of-00001',\n",
        "            vgg_dir + '/variables/variables.index',\n",
        "            vgg_dir + '/saved_model.pb']\n",
        "\n",
        "        missing_vgg_files = [vgg_file for vgg_file in vgg_files \\\n",
        "                             if not os.path.exists(vgg_file)]\n",
        "\n",
        "        if missing_vgg_files:\n",
        "            if os.path.exists(vgg_dir):\n",
        "                shutil.rmtree(vgg_dir)\n",
        "            os.makedirs(vgg_dir)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Download vgg\n",
        "            #-------------------------------------------------------------------\n",
        "            url = 'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip'\n",
        "            if not os.path.exists(vgg_archive):\n",
        "                if callable(progress_hook):\n",
        "                    urlretrieve(url, vgg_archive, progress_hook)\n",
        "                else:\n",
        "                    with DLProgress(unit='B', unit_scale=True, miniters=1) as pbar:\n",
        "                        urlretrieve(url, vgg_archive, pbar.hook)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Extract vgg\n",
        "            #-------------------------------------------------------------------\n",
        "            zip_archive = zipfile.ZipFile(vgg_archive, 'r')\n",
        "            zip_archive.extractall(vgg_dir)\n",
        "            zip_archive.close()\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __load_vgg(self, vgg_dir):\n",
        "        sess = self.session\n",
        "        graph = tf.saved_model.loader.load(sess, ['vgg16'], vgg_dir+'/vgg')\n",
        "        self.image_input = sess.graph.get_tensor_by_name('image_input:0')\n",
        "        self.keep_prob   = sess.graph.get_tensor_by_name('keep_prob:0')\n",
        "        self.vgg_conv4_3 = sess.graph.get_tensor_by_name('conv4_3/Relu:0')\n",
        "        self.vgg_conv5_3 = sess.graph.get_tensor_by_name('conv5_3/Relu:0')\n",
        "        self.vgg_fc6_w   = sess.graph.get_tensor_by_name('fc6/weights:0')\n",
        "        self.vgg_fc6_b   = sess.graph.get_tensor_by_name('fc6/biases:0')\n",
        "        self.vgg_fc7_w   = sess.graph.get_tensor_by_name('fc7/weights:0')\n",
        "        self.vgg_fc7_b   = sess.graph.get_tensor_by_name('fc7/biases:0')\n",
        "\n",
        "        layers = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1',\n",
        "                  'conv3_2', 'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3',\n",
        "                  'conv5_1', 'conv5_2', 'conv5_3']\n",
        "\n",
        "        for l in layers:\n",
        "            self.l2_loss += sess.graph.get_tensor_by_name(l+'/L2Loss:0')\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_vgg_mods(self):\n",
        "        self.mod_pool5 = tf.nn.max_pool(self.vgg_conv5_3, ksize=[1, 3, 3, 1],\n",
        "                                        strides=[1, 1, 1, 1], padding='SAME',\n",
        "                                        name='mod_pool5')\n",
        "\n",
        "        with tf.variable_scope('mod_conv6'):\n",
        "            x = tf.nn.conv2d(self.mod_pool5, self.vgg_fc6_w,\n",
        "                             strides=[1, 1, 1, 1], padding='SAME')\n",
        "            x = tf.nn.bias_add(x, self.vgg_fc6_b)\n",
        "            self.mod_conv6 = tf.nn.relu(x)\n",
        "            self.l2_loss += tf.nn.l2_loss(self.vgg_fc6_w)\n",
        "\n",
        "        with tf.variable_scope('mod_conv7'):\n",
        "            x = tf.nn.conv2d(self.mod_conv6, self.vgg_fc7_w,\n",
        "                             strides=[1, 1, 1, 1], padding='SAME')\n",
        "            x = tf.nn.bias_add(x, self.vgg_fc7_b)\n",
        "            x = tf.nn.relu(x)\n",
        "            self.mod_conv7 = x\n",
        "            self.l2_loss += tf.nn.l2_loss(self.vgg_fc7_w)\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_vgg_mods_a_trous(self):\n",
        "        sess = self.session\n",
        "\n",
        "        self.mod_pool5 = tf.nn.max_pool(self.vgg_conv5_3, ksize=[1, 3, 3, 1],\n",
        "                                        strides=[1, 1, 1, 1], padding='SAME',\n",
        "                                        name='mod_pool5')\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Modified conv6\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('mod_conv6'):\n",
        "            #-------------------------------------------------------------------\n",
        "            # Decimate the weights\n",
        "            #-------------------------------------------------------------------\n",
        "            orig_w, orig_b = sess.run([self.vgg_fc6_w, self.vgg_fc6_b])\n",
        "            mod_w = np.zeros((3, 3, 512, 1024))\n",
        "            mod_b = np.zeros(1024)\n",
        "\n",
        "            for i in range(1024):\n",
        "                mod_b[i] = orig_b[4*i]\n",
        "                for h in range(3):\n",
        "                    for w in range(3):\n",
        "                        mod_w[h, w, :, i] = orig_w[3*h, 3*w, :, 4*i]\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Build the feature map\n",
        "            #-------------------------------------------------------------------\n",
        "            w = array2tensor(mod_w, 'filter')\n",
        "            b = array2tensor(mod_b, 'biases')\n",
        "            x = tf.nn.atrous_conv2d(self.mod_pool5, w, rate=6, padding='SAME')\n",
        "            x = tf.nn.bias_add(x, b)\n",
        "            x = tf.nn.relu(x)\n",
        "            self.mod_conv6 = x\n",
        "            self.l2_loss += tf.nn.l2_loss(w)\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Modified conv7\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('mod_conv7'):\n",
        "            #-------------------------------------------------------------------\n",
        "            # Decimate the weights\n",
        "            #-------------------------------------------------------------------\n",
        "            orig_w, orig_b = sess.run([self.vgg_fc7_w, self.vgg_fc7_b])\n",
        "            mod_w = np.zeros((1, 1, 1024, 1024))\n",
        "            mod_b = np.zeros(1024)\n",
        "\n",
        "            for i in range(1024):\n",
        "                mod_b[i] = orig_b[4*i]\n",
        "                for j in range(1024):\n",
        "                    mod_w[:, :, j, i] = orig_w[:, :, 4*j, 4*i]\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Build the feature map\n",
        "            #-------------------------------------------------------------------\n",
        "            w = array2tensor(mod_w, 'filter')\n",
        "            b = array2tensor(mod_b, 'biases')\n",
        "            x = tf.nn.conv2d(self.mod_conv6, w, strides=[1, 1, 1, 1],\n",
        "                             padding='SAME')\n",
        "            x = tf.nn.bias_add(x, b)\n",
        "            x = tf.nn.relu(x)\n",
        "            self.mod_conv7 = x\n",
        "            self.l2_loss += tf.nn.l2_loss(w)\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __with_loss(self, x, l2_loss):\n",
        "        self.l2_loss += l2_loss\n",
        "        return x\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_ssd_layers(self):\n",
        "        stride10 = 1\n",
        "        padding10 = 'VALID'\n",
        "        if len(self.preset.maps) >= 7:\n",
        "            stride10 = 2\n",
        "            padding10 = 'SAME'\n",
        "\n",
        "        x, l2  = conv_map(self.mod_conv7,    256, 1, 1, 'conv8_1')\n",
        "        self.ssd_conv8_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv8_1,  512, 3, 2, 'conv8_2')\n",
        "        self.ssd_conv8_2 = self.__with_loss(x, l2)\n",
        "        x, l2  = conv_map(self.ssd_conv8_2,  128, 1, 1, 'conv9_1')\n",
        "        self.ssd_conv9_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv9_1,  256, 3, 2, 'conv9_2')\n",
        "        self.ssd_conv9_2 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv9_2,  128, 1, 1, 'conv10_1')\n",
        "        self.ssd_conv10_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv10_1, 256, 3, stride10, 'conv10_2', padding10)\n",
        "        self.ssd_conv10_2 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv10_2, 128, 1, 1, 'conv11_1')\n",
        "        self.ssd_conv11_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv11_1, 256, 3, 1, 'conv11_2', 'VALID')\n",
        "        self.ssd_conv11_2 = self.__with_loss(x, l2)\n",
        "\n",
        "        if len(self.preset.maps) < 7:\n",
        "            return\n",
        "\n",
        "        x, l2 = conv_map(self.ssd_conv11_2, 128, 1, 1, 'conv12_1')\n",
        "        paddings = [[0, 0], [0, 1], [0, 1], [0, 0]]\n",
        "        x = tf.pad(x, paddings, \"CONSTANT\")\n",
        "        self.ssd_conv12_1 = self.__with_loss(x, l2)\n",
        "        x, l2 = conv_map(self.ssd_conv12_1, 256, 3, 1, 'conv12_2', 'VALID')\n",
        "        self.ssd_conv12_2 = self.__with_loss(x, l2)\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_norms(self):\n",
        "        x = l2_normalization(self.vgg_conv4_3, 20, 512, 'l2_norm_conv4_3')\n",
        "        self.norm_conv4_3 = x\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __select_feature_maps(self):\n",
        "        self.__maps = [\n",
        "            self.norm_conv4_3,\n",
        "            self.mod_conv7,\n",
        "            self.ssd_conv8_2,\n",
        "            self.ssd_conv9_2,\n",
        "            self.ssd_conv10_2,\n",
        "            self.ssd_conv11_2]\n",
        "\n",
        "        if len(self.preset.maps) == 7:\n",
        "            self.__maps.append(self.ssd_conv12_2)\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_classifiers(self):\n",
        "        with tf.variable_scope('classifiers'):\n",
        "            self.__classifiers = []\n",
        "            for i in range(len(self.__maps)):\n",
        "                fmap = self.__maps[i]\n",
        "                map_size = self.preset.maps[i].size\n",
        "                for j in range(2+len(self.preset.maps[i].aspect_ratios)):\n",
        "                    name = 'classifier{}_{}'.format(i, j)\n",
        "                    clsfier, l2 = classifier(fmap, self.num_vars, map_size, name)\n",
        "                    self.__classifiers.append(self.__with_loss(clsfier, l2))\n",
        "\n",
        "        with tf.variable_scope('output'):\n",
        "            output      = tf.concat(self.__classifiers, axis=1, name='output')\n",
        "            self.logits = output[:,:,:self.num_classes]\n",
        "\n",
        "        with tf.variable_scope('result'):\n",
        "            self.classifier = tf.nn.softmax(self.logits)\n",
        "            self.locator    = output[:,:,self.num_classes:]\n",
        "            self.result     = tf.concat([self.classifier, self.locator],\n",
        "                                        axis=-1, name='result')\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_optimizer(self, learning_rate=0.001, weight_decay=0.0005,\n",
        "                        momentum=0.9, global_step=None):\n",
        "\n",
        "        self.labels = tf.placeholder(tf.float32, name='labels',\n",
        "                                    shape=[None, None, self.num_vars])\n",
        "\n",
        "        with tf.variable_scope('ground_truth'):\n",
        "            #-------------------------------------------------------------------\n",
        "            # Split the ground truth tensor\n",
        "            #-------------------------------------------------------------------\n",
        "            # Classification ground truth tensor\n",
        "            # Shape: (batch_size, num_anchors, num_classes)\n",
        "            gt_cl = self.labels[:,:,:self.num_classes]\n",
        "\n",
        "            # Localization ground truth tensor\n",
        "            # Shape: (batch_size, num_anchors, 4)\n",
        "            gt_loc = self.labels[:,:,self.num_classes:]\n",
        "\n",
        "            # Batch size\n",
        "            # Shape: scalar\n",
        "            batch_size = tf.shape(gt_cl)[0]\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute match counters\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('match_counters'):\n",
        "            # Number of anchors per sample\n",
        "            # Shape: (batch_size)\n",
        "            total_num = tf.ones([batch_size], dtype=tf.int64) * \\\n",
        "                        tf.to_int64(self.preset.num_anchors)\n",
        "\n",
        "            # Number of negative (not-matched) anchors per sample, computed\n",
        "            # by counting boxes of the background class in each sample.\n",
        "            # Shape: (batch_size)\n",
        "            negatives_num = tf.count_nonzero(gt_cl[:,:,-1], axis=1)\n",
        "\n",
        "            # Number of positive (matched) anchors per sample\n",
        "            # Shape: (batch_size)\n",
        "            positives_num = total_num-negatives_num\n",
        "\n",
        "            # Number of positives per sample that is division-safe\n",
        "            # Shape: (batch_size)\n",
        "            positives_num_safe = tf.where(tf.equal(positives_num, 0),\n",
        "                                          tf.ones([batch_size])*10e-15,\n",
        "                                          tf.to_float(positives_num))\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute masks\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('match_masks'):\n",
        "            # Boolean tensor determining whether an anchor is a positive\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            positives_mask = tf.equal(gt_cl[:,:,-1], 0)\n",
        "\n",
        "            # Boolean tensor determining whether an anchor is a negative\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_mask = tf.logical_not(positives_mask)\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute the confidence loss\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('confidence_loss'):\n",
        "            # Cross-entropy tensor - all of the values are non-negative\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=gt_cl,\n",
        "                                                            logits=self.logits)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Sum up the loss of all the positive anchors\n",
        "            #-------------------------------------------------------------------\n",
        "            # Positives - the loss of negative anchors is zeroed out\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            positives = tf.where(positives_mask, ce, tf.zeros_like(ce))\n",
        "\n",
        "            # Total loss of positive anchors\n",
        "            # Shape: (batch_size)\n",
        "            positives_sum = tf.reduce_sum(positives, axis=-1)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Figure out what the negative anchors with highest confidence loss\n",
        "            # are\n",
        "            #-------------------------------------------------------------------\n",
        "            # Negatives - the loss of positive anchors is zeroed out\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives = tf.where(negatives_mask, ce, tf.zeros_like(ce))\n",
        "\n",
        "            # Top negatives - sorted confience loss with the highest one first\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_top = tf.nn.top_k(negatives, self.preset.num_anchors)[0]\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Fugure out what the number of negatives we want to keep is\n",
        "            #-------------------------------------------------------------------\n",
        "            # Maximum number of negatives to keep per sample - we keep at most\n",
        "            # 3 times as many as we have positive anchors in the sample\n",
        "            # Shape: (batch_size)\n",
        "            negatives_num_max = tf.minimum(negatives_num, 3*positives_num)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Mask out superfluous negatives and compute the sum of the loss\n",
        "            #-------------------------------------------------------------------\n",
        "            # Transposed vector of maximum negatives per sample\n",
        "            # Shape (batch_size, 1)\n",
        "            negatives_num_max_t = tf.expand_dims(negatives_num_max, 1)\n",
        "\n",
        "            # Range tensor: [0, 1, 2, ..., num_anchors-1]\n",
        "            # Shape: (num_anchors)\n",
        "            rng = tf.range(0, self.preset.num_anchors, 1)\n",
        "\n",
        "            # Row range, the same as above, but int64 and a row of a matrix\n",
        "            # Shape: (1, num_anchors)\n",
        "            range_row = tf.to_int64(tf.expand_dims(rng, 0))\n",
        "\n",
        "            # Mask of maximum negatives - first `negative_num_max` elements\n",
        "            # in corresponding row are `True`, the rest is false\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_max_mask = tf.less(range_row, negatives_num_max_t)\n",
        "\n",
        "            # Max negatives - all the positives and superfluous negatives are\n",
        "            # zeroed out.\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            negatives_max = tf.where(negatives_max_mask, negatives_top,\n",
        "                                     tf.zeros_like(negatives_top))\n",
        "\n",
        "            # Sum of max negatives for each sample\n",
        "            # Shape: (batch_size)\n",
        "            negatives_max_sum = tf.reduce_sum(negatives_max, axis=-1)\n",
        "\n",
        "            #-------------------------------------------------------------------\n",
        "            # Compute the confidence loss for each element\n",
        "            #-------------------------------------------------------------------\n",
        "            # Total confidence loss for each sample\n",
        "            # Shape: (batch_size)\n",
        "            confidence_loss = tf.add(positives_sum, negatives_max_sum)\n",
        "\n",
        "            # Total confidence loss normalized by the number of positives\n",
        "            # per sample\n",
        "            # Shape: (batch_size)\n",
        "            confidence_loss = tf.where(tf.equal(positives_num, 0),\n",
        "                                       tf.zeros([batch_size]),\n",
        "                                       tf.div(confidence_loss,\n",
        "                                              positives_num_safe))\n",
        "\n",
        "            # Mean confidence loss for the batch\n",
        "            # Shape: scalar\n",
        "            self.confidence_loss = tf.reduce_mean(confidence_loss,\n",
        "                                                  name='confidence_loss')\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute the localization loss\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('localization_loss'):\n",
        "            # Element-wise difference between the predicted localization loss\n",
        "            # and the ground truth\n",
        "            # Shape: (batch_size, num_anchors, 4)\n",
        "            loc_diff = tf.subtract(self.locator, gt_loc)\n",
        "\n",
        "            # Smooth L1 loss\n",
        "            # Shape: (batch_size, num_anchors, 4)\n",
        "            loc_loss = smooth_l1_loss(loc_diff)\n",
        "\n",
        "            # Sum of localization losses for each anchor\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            loc_loss_sum = tf.reduce_sum(loc_loss, axis=-1)\n",
        "\n",
        "            # Positive locs - the loss of negative anchors is zeroed out\n",
        "            # Shape: (batch_size, num_anchors)\n",
        "            positive_locs = tf.where(positives_mask, loc_loss_sum,\n",
        "                                     tf.zeros_like(loc_loss_sum))\n",
        "\n",
        "            # Total loss of positive anchors\n",
        "            # Shape: (batch_size)\n",
        "            localization_loss = tf.reduce_sum(positive_locs, axis=-1)\n",
        "\n",
        "            # Total localization loss normalized by the number of positives\n",
        "            # per sample\n",
        "            # Shape: (batch_size)\n",
        "            localization_loss = tf.where(tf.equal(positives_num, 0),\n",
        "                                         tf.zeros([batch_size]),\n",
        "                                         tf.div(localization_loss,\n",
        "                                                positives_num_safe))\n",
        "\n",
        "            # Mean localization loss for the batch\n",
        "            # Shape: scalar\n",
        "            self.localization_loss = tf.reduce_mean(localization_loss,\n",
        "                                                    name='localization_loss')\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Compute total loss\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('total_loss'):\n",
        "            # Sum of the localization and confidence loss\n",
        "            # Shape: (batch_size)\n",
        "            self.conf_and_loc_loss = tf.add(self.confidence_loss,\n",
        "                                            self.localization_loss,\n",
        "                                            name='sum_losses')\n",
        "\n",
        "            # L2 loss\n",
        "            # Shape: scalar\n",
        "            self.l2_loss = tf.multiply(weight_decay, self.l2_loss,\n",
        "                                       name='l2_loss')\n",
        "\n",
        "            # Final loss\n",
        "            # Shape: scalar\n",
        "            self.loss = tf.add(self.conf_and_loc_loss, self.l2_loss,\n",
        "                               name='loss')\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Build the optimizer\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('optimizer'):\n",
        "            optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "            optimizer = optimizer.minimize(self.loss, global_step=global_step,\n",
        "                                           name='optimizer')\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Store the tensors\n",
        "        #-----------------------------------------------------------------------\n",
        "        self.optimizer = optimizer\n",
        "        self.losses = {\n",
        "            'total': self.loss,\n",
        "            'localization': self.localization_loss,\n",
        "            'confidence': self.confidence_loss,\n",
        "            'l2': self.l2_loss\n",
        "        }\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def __build_names(self):\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Names of the original and new scopes\n",
        "        #-----------------------------------------------------------------------\n",
        "        self.original_scopes = [\n",
        "            'conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2',\n",
        "            'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2',\n",
        "            'conv5_3', 'mod_conv6', 'mod_conv7'\n",
        "        ]\n",
        "\n",
        "        self.new_scopes = [\n",
        "            'conv8_1', 'conv8_2', 'conv9_1', 'conv9_2', 'conv10_1', 'conv10_2',\n",
        "            'conv11_1', 'conv11_2'\n",
        "        ]\n",
        "\n",
        "        if len(self.preset.maps) == 7:\n",
        "            self.new_scopes += ['conv12_1', 'conv12_2']\n",
        "\n",
        "        for i in range(len(self.preset.maps)):\n",
        "            for j in range(2+len(self.preset.maps[i].aspect_ratios)):\n",
        "                self.new_scopes.append('classifiers/classifier{}_{}'.format(i, j))\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    def build_summaries(self, restore):\n",
        "        if restore:\n",
        "            return self.session.graph.get_tensor_by_name('net_summaries/net_summaries:0')\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Build the filter summaries\n",
        "        #-----------------------------------------------------------------------\n",
        "        names = self.original_scopes + self.new_scopes\n",
        "        sess = self.session\n",
        "        with tf.variable_scope('filter_summaries'):\n",
        "            summaries = []\n",
        "            for name in names:\n",
        "                tensor = sess.graph.get_tensor_by_name(name+'/filter:0')\n",
        "                summary = tf.summary.histogram(name, tensor)\n",
        "                summaries.append(summary)\n",
        "\n",
        "        #-----------------------------------------------------------------------\n",
        "        # Scale summary\n",
        "        #-----------------------------------------------------------------------\n",
        "        with tf.variable_scope('scale_summary'):\n",
        "            tensor = sess.graph.get_tensor_by_name('l2_norm_conv4_3/scale:0')\n",
        "            summary = tf.summary.histogram('l2_norm_conv4_3', tensor)\n",
        "            summaries.append(summary)\n",
        "\n",
        "        return tf.summary.merge(summaries, name='net_summaries')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ_l9W8M5ePN"
      },
      "source": [
        "## Практическое задание\n",
        "\n",
        "<ol>\n",
        "    <li>Сделайте краткий обзор какой-нибудь научной работы посвященной тому или иному алгоритму для object detection, который не рассматривался на уроке. Проведите анализ: Чем отличается выбранная вами на рассмотрение архитектура нейронной сети от других архитектур? В чем плюсы и минусы данной архитектуры? Какие могут возникнуть трудности при применении данной архитектуры на практике?\n",
        "    </li> \n",
        "    <li>Запустите детектор (ssdMobile_v2 или faster_rcnn, или любой другой детектор) для своей картинки и попробуйте найти 10 объектов, 100 объектов.\n",
        "    </li>\n",
        "    <li>* Ссылка на репозиторий с полным кодом для обучения ssd нейросети - https://github.com/sergeyveneckiy/ssd-tensorflow. Попробуйте улучшить точность ее работы и напишите отчет, что вы пробовали изменить в ее параметрах и как это отражалось на процессе обучения нейронной сети. \n",
        "        Обратите внимание! Мин. сист. требования для запуска данного проекта - это минимум 8 Gb ОЗУ. Если у вас недостаточно мощности компьютера, то вы можете просто изучить содержимое исходного кода и датасета данного проекта.</li>\n",
        "\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiGyjf1M5ePP"
      },
      "source": [
        "## Дополнительные материалы\n",
        "\n",
        "\n",
        "1. [Оригинальная научная статья по MS COCO dataset](https://arxiv.org/pdf/1405.0312.pdf)\n",
        "2. [Оригинальная научная статья по R-CNN](https://arxiv.org/pdf/1311.2524.pdf)\n",
        "3. [Оригинальная научная статья по Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)\n",
        "4. [Оригинальная научная статья по Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf)\n",
        "5. [Оригинальная научная статья по YOLO](https://arxiv.org/pdf/1506.02640.pdf)\n",
        "6. [Оригинальная научная статья по SSD](https://arxiv.org/pdf/1512.02325.pdf)\n",
        "7. [Оригинальная научная статья по Mask R-CNN](https://arxiv.org/pdf/2001.05566.pdf)\n",
        "7. [mAP (mean Average Precision) for Object Detection](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnpfYxC15ePQ"
      },
      "source": [
        "## Используемая литература \n",
        "\n",
        "Для подготовки данного методического пособия были использованы следующие ресурсы:\n",
        "<ol>\n",
        "    <li>https://github.com/ljanyst/ssd-tensorflow</li>\n",
        "    <li>Recent Advances in Deep Learning for Object Detection. Xiongwei Wu, Doyen Sahoo, Steven C.H. Hoi. 2019</li>    \n",
        "    <li>Object Detection with Deep Learning: A Review. Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, Xindong Wu. 2019\n",
        "</li>\n",
        "    <li>Object Detection in 20 Years: A Survey. Zhengxia Zou (1), Zhenwei Shi (2), Yuhong Guo (3 and 4), Jieping Ye (1 and 4) ((1) University of Michigan, (2) Beihang University, (3) Carleton University, (4) DiDi Chuxing). 2019</li>\n",
        "    <li>Википедия</li>  \n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmYBVDhbGXIn"
      },
      "source": [
        "## Определения\n",
        "\n",
        "**Intersection over Union (IoU)** — это мера величины перекрытия между двумя прямоугольниками). Он вычисляет размер перекрытия между двумя объектами, деленный на общую площадь двух объединенных объектов.\n",
        "\n",
        "**mean Average Precision (mAP)** — среднее значение максимумов precision'ов при различных значениях recall'ов.\n"
      ]
    }
  ]
}